# Upload Configuration (centralized - used by all services)
# Single source of truth: 512MB = 536870912 bytes
# All services derive their config from this value
MAX_UPLOAD_SIZE_BYTES=536870912

# Nginx Port Configuration
# Default: 80 (HTTP standard port)
# Change if port 80 is already in use on your system
NGINX_PORT=80

# LLM Configuration (Local LLM Server)
# Use a local LLM server (LM Studio, Ollama) or OpenAI API
LLM_API_BASE_URL=http://localhost:1234/v1
LLM_API_KEY=
LLM_MODEL=Qwen2.5-14B-Coder-Instruct
LLM_TEMPERATURE=0.7

# LLM_MAX_TOKENS: Maximum tokens for LLM responses
# - Auto-detects model's context length from /v1/models endpoint
# - Uses minimum of: configured value OR 80% of model's context length
# - Serves as fallback if auto-detection fails
# - Acts as cost control/upper limit even if model supports more
# Recommended: 8000-16000 for most models (adjust based on your needs)
LLM_MAX_TOKENS=8000
